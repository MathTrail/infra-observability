# OpenTelemetry Collector for MathTrail
# Role: Smart gateway between Dapr sidecars and Grafana LGTM stack

mode: deployment

replicaCount: 2

image:
  repository: otel/opentelemetry-collector-contrib
  tag: "0.145.0"
  pullPolicy: IfNotPresent

# Resource configuration
resources:
  requests:
    cpu: "200m"
    memory: "256Mi"
  limits:
    cpu: "1000m"
    memory: "1Gi"

# Service configuration
service:
  type: ClusterIP

# Ports for receivers
ports:
  # OTLP gRPC
  otlp:
    enabled: true
    containerPort: 4317
    servicePort: 4317
    protocol: TCP

  # OTLP HTTP
  otlp-http:
    enabled: true
    containerPort: 4318
    servicePort: 4318
    protocol: TCP

  # Zipkin (for Dapr)
  zipkin:
    enabled: true
    containerPort: 9411
    servicePort: 9411
    protocol: TCP

  # Prometheus metrics (collector self-monitoring)
  prometheus:
    enabled: true
    containerPort: 8888
    servicePort: 8888
    protocol: TCP

  # Health check
  health-check:
    enabled: true
    containerPort: 13133
    servicePort: 13133
    protocol: TCP

# Collector configuration
config:
  receivers:
    # OTLP receiver for metrics and logs from services
    otlp:
      protocols:
        grpc:
          endpoint: 0.0.0.0:4317
        http:
          endpoint: 0.0.0.0:4318

    # Zipkin receiver for Dapr traces
    zipkin:
      endpoint: 0.0.0.0:9411

    # Prometheus receiver (scrape collector's own metrics)
    prometheus:
      config:
        scrape_configs:
          - job_name: 'otel-collector'
            scrape_interval: 30s
            static_configs:
              - targets: ['localhost:8888']

  processors:
    # Batch processor for efficiency
    batch:
      timeout: 10s
      send_batch_size: 1024

    # Memory limiter to prevent OOM
    memory_limiter:
      check_interval: 5s
      limit_percentage: 80
      spike_limit_percentage: 25

    # k8sattributes: Enrich telemetry with Kubernetes metadata
    k8sattributes:
      auth_type: "serviceAccount"
      passthrough: false
      extract:
        metadata:
          - k8s.namespace.name
          - k8s.deployment.name
          - k8s.statefulset.name
          - k8s.daemonset.name
          - k8s.cronjob.name
          - k8s.job.name
          - k8s.node.name
          - k8s.pod.name
          - k8s.pod.uid
          - k8s.pod.start_time
        labels:
          - tag_name: app.kubernetes.io/name
            key: app.kubernetes.io/name
            from: pod
          - tag_name: app.kubernetes.io/instance
            key: app.kubernetes.io/instance
            from: pod
          - tag_name: app.kubernetes.io/part-of
            key: app.kubernetes.io/part-of
            from: pod
      pod_association:
        - sources:
            - from: resource_attribute
              name: k8s.pod.ip
        - sources:
            - from: resource_attribute
              name: k8s.pod.uid
        - sources:
            - from: connection

  exporters:
    # Export to Grafana Alloy (LGTM ingestion point)
    otlp/alloy:
      endpoint: "lgtm-alloy-receiver.monitoring.svc.cluster.local:4317"
      tls:
        insecure: true
      sending_queue:
        enabled: true
        num_consumers: 10
        queue_size: 5000
      retry_on_failure:
        enabled: true
        initial_interval: 5s
        max_interval: 30s
        max_elapsed_time: 300s

    # Debug exporter for troubleshooting
    debug:
      verbosity: detailed
      sampling_initial: 5
      sampling_thereafter: 200

  extensions:
    health_check:
      endpoint: 0.0.0.0:13133

  service:
    extensions: [health_check]
    pipelines:
      # Traces pipeline: Zipkin + OTLP → k8s enrichment → Alloy
      traces:
        receivers: [otlp, zipkin]
        processors: [memory_limiter, k8sattributes, batch]
        exporters: [otlp/alloy, debug]

      # Metrics pipeline: OTLP → k8s enrichment → Alloy
      metrics:
        receivers: [otlp, prometheus]
        processors: [memory_limiter, k8sattributes, batch]
        exporters: [otlp/alloy, debug]

      # Logs pipeline: OTLP → k8s enrichment → Alloy
      logs:
        receivers: [otlp]
        processors: [memory_limiter, k8sattributes, batch]
        exporters: [otlp/alloy, debug]

# RBAC - required for k8sattributes processor
serviceAccount:
  create: true
  name: otel-collector

# ClusterRole for k8sattributes processor
clusterRole:
  create: true
  rules:
    - apiGroups: [""]
      resources:
        - nodes
        - nodes/proxy
        - services
        - endpoints
        - pods
        - namespaces
      verbs: ["get", "list", "watch"]
    - apiGroups: ["apps"]
      resources:
        - replicasets
        - deployments
        - daemonsets
        - statefulsets
      verbs: ["get", "list", "watch"]
    - apiGroups: ["batch"]
      resources:
        - jobs
        - cronjobs
      verbs: ["get", "list", "watch"]

# Pod anti-affinity for HA
affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                  - opentelemetry-collector
          topologyKey: kubernetes.io/hostname
